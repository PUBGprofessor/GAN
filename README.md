## 版本v1:

1. 数据集同时使用MNIST数据集和G.gteImage(),每次根据索引取出一张，然后由datasetloader每次取batchsize次，G.forward()无法并行，导致取数据漫

2. G全卷积层，初始随机传入一个大的x数组，可能占内存大
3. 损失函数使用均方误差
4. D和G训练比1：200

训练过慢，无法实用

## 版本v2：

1. 数据集只用MNIST，batch下取数据快
2. G两层全连接＋一层卷积
3. 使用手写的交叉熵损失
4. D和G训练比 1：300

训练速度还好，D损失下降快，训练充分；G训练不出来，输出的随机图片

## 版本v3.0：

总结问题：

1. D 收敛得太快，直接就把 G 生成的图片看穿了，D最后一层的激活函数为sigmod，训练太好导致每次返回（在sigmod的左右两侧）梯度消失
2. 让 D **不要太自信**，反而有助于 G 的训练

解决：

1. 减少D的网络层数，增加G网络层数，使用假标签（**真实标签为 1** 的地方改成 **0.9**，**假标签 0** 改为 **0.1**），防止sigmod梯度消失
2. 加入假标签，减慢训练D的能力
3. D和G训练比 1：30

## 版本v3.1：

1. lossG使用 BCEWithLogitsLoss 计算损失
2. D和G训练比 1：30